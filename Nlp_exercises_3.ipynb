{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjWSewNbeT5j"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "def get_file_data(stop_word_removal='no'):\n",
        "    # Loading the file\n",
        "    with open('Dataset to compute Word Embeddings (Tiny).txt') as f:\n",
        "        file_contents = f.read()\n",
        "\n",
        "    # Word extraction\n",
        "    words = re.findall(r'\\b\\w+\\b', file_contents)\n",
        "\n",
        "    # Optionally remove stop words\n",
        "    if stop_word_removal == 'yes':\n",
        "        stop_words = set(stopwords.words('english'))  # Assuming English text\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Joining words into a string again\n",
        "    text = ' '.join(words)\n",
        "    \n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "wO2Aido5ecIb",
        "outputId": "83e8bee3-04b5-45eb-b136-25e8a59f13ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RAte the Black Coffee chronicle three out of 6 stars She me movie times at Mann Theatres Please book a brasserie restaurant for eight in Ireland Find the novel WWE Legends of WrestleMania I rate American History A Survey a 5 Play Circus Farm by Deana Carter Play Pandora on Last Fm play Party Ben on Slacker Play some music from the last album of 1988 on Lastfm What is the forecast for Dec 1st 2036 in Keeneland What is the weather going to be like on st patrick s day Tell me when it will be warmer in Woods Hole Oklahoma add this tune to my fresh finds hiptronix playlist What are the Mann Theatres showtimes for Secret Sunshine Add the avispa track to my Bass Gaming playlist book a spot for krista yolanda and I in New Mexico Is Babar King of the Elephants playing Is it possible to see Tube at the closest movie theatre Will there be rainfall at one PM in Catahoula Add david cole to an instrumental sunday I feel like this essay deserves four stars Where is Nichiren to Mōko Daishūrai playing Find the movie schedule at twelve AM What song is Detective Conan Dimensional Sniper find Thyme Travel what is the chilly forecast for Mustoe North Carolina Show me the movie schedules for movies playing around here today I m looking for the television show titled Justice League I need a list of places in the area where I can see animated movies in six minutes Tell me the weather forecast for Northern Mariana Islands one second from now Please look up the work Bachelor Pad Go to The Devil in the Deal 50 Secrets to Successful Dealmaking I want seats for four at a place in RI Can i see the Encounters Add Tranquility to the Latin Pop Rising playlist what is the forecast for 6 am in Aruba Add ricky nelson to my Classical X playist rate Orion in the Dying Time four stars Add name Me the Rhythm to my chill out music Is it supposed to hail in NY I want maximilian mutzke added to Acoustic Concentration rate The Cry of the Owl a four I want the photograph of Walt Before Mickey Can I get the movie schedules for Loews Cineplex I give One Clear Call zero out of 6 points Will it be warmer in the District Of Columbia on may 25 2033 rate this essay zero out of 6 Play 1958 music I want to hear some theme music by Edsel Dope The Revolution Script should have a rating value of three and a best rating of 6 Give one of 6 points to Who Will Cry When You Die this next essay is worth five Book a reservation for an indonesian brasserie on 9 13 2017 Book a table at a bar in Moody for deloris ester and petra alvarez Use netflix to play a record by Emil Gilels from year 2015 Play Sergei Anatoljewitsch Kurjochin s music on Groove Shark Play music from 1964 Play my entertaining playlist Give this book a 5 stars rating Find a table at a bar for milagros and I in Mount Pocono Rate this album 2 out of 6 Will there be alot of wind on March 13th in Lost Creek Bahrain I would rate this novel a four give four out of 6 star to this novel give 0 out of 6 points to this essay I would like to book a brasserie for nine in AR find The World Is a Game rate Shockscape five stars rate this textbook a four play pandora tracks by Akhtar Sadmani I need to find the saga Trail of the Yukon Rate Lords of the Rim zero stars Play Eddie Meduza from the thirties Will it be cold in Wheatley Provincial Park play the song Shine A Light Will it rain in Paisley The current essay gets four points what movies are playing at the Alamo Drafthouse Cinema add tune to my instrumental funk playlist Play music on Itunes I rate The Blood of Others series only five points Find the show Manthan Add this carlos libedinsky song to my zen focus playlist add Paloma negra to my Funky Jams list The Evil Experiment gets zero out of 6 points Add the song to my R B Movement playlist this saga is definitely worth 4 stars Book a reservation for my mommy and I at a restaurant in Central African Republic find movie times nearby for animated movies book a restaurant in Puerto Rico find Plitt Theatres showing animated movies Play the newest music by Gladys Knight show me the schedule of Letters from a Porcupine in Alamo Drafthouse Cinema What will the weather be like when I get out of my afternoon meeting Add an album to my Sylvia Plath playlist Book a reservation for a restaurant in Wadsworth with parking add this tune to Escapada Play Ashita E by Ian Anderson give 3 out of 6 points to The Secrets of Love Is Waldorf Astoria more luxurious than the Four Seasons'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dictionary_data(text):\n",
        "    # Here we're converting the text to a list of words\n",
        "    # and then creating a dictionary from it.\n",
        "    corpus = text.split()\n",
        "    word_to_index = {word: index for index, word in enumerate(set(corpus))}\n",
        "    index_to_word = {index: word for word, index in word_to_index.items()}\n",
        "\n",
        "    return word_to_index, index_to_word, corpus, len(word_to_index), len(corpus)\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "HsALfL9Mel0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlAAbNz8e6Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_one_hot_vectors(target_word, context_words, vocab_size, word_to_index):\n",
        "    # Initialize the vectors with zeros\n",
        "    target_word_vector = np.zeros(vocab_size)\n",
        "    context_word_vector = np.zeros(vocab_size)\n",
        "\n",
        "    # Set the word ID to 1\n",
        "    target_word_vector[word_to_index[target_word]] = 1\n",
        "\n",
        "    # Do the same for context words\n",
        "    for word in context_words:\n",
        "        context_word_vector[word_to_index[word]] = 1\n",
        "\n",
        "    return target_word_vector, context_word_vector\n",
        "\n",
        "target_word = \"chronicle\"\n",
        "context_words = [\"Black\", \"Coffee\", \"three\", \"out\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kumGFCeSfBEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(corpus, window_size, vocab_size, word_to_index, length_of_corpus, sample=None):\n",
        "    training_data =  []\n",
        "    training_sample_words =  []\n",
        "\n",
        "    for i, word in enumerate(corpus):\n",
        "        index_target_word = i\n",
        "        target_word = word\n",
        "        context_words = []\n",
        "\n",
        "        #when target word is the first word\n",
        "        if i == 0:  \n",
        "            # trgt_word_index:(0), ctxt_word_index:(1,2)\n",
        "            context_words = [corpus[x] for x in range(i + 1 , window_size + 1)] \n",
        "\n",
        "        #when target word is the last word\n",
        "        elif i == len(corpus)-1:\n",
        "            # trgt_word_index:(9), ctxt_word_index:(8,7), length_of_corpus = 10\n",
        "            context_words = [corpus[x] for x in range(length_of_corpus - 2 ,length_of_corpus -2 - window_size  , -1 )]\n",
        "\n",
        "        #When target word is the middle word\n",
        "        else:\n",
        "            #Before the middle target word\n",
        "            before_target_word_index = index_target_word - 1\n",
        "            for x in range(before_target_word_index, before_target_word_index - window_size , -1):\n",
        "                if x >=0:\n",
        "                    context_words.extend([corpus[x]])\n",
        "\n",
        "            #After the middle target word\n",
        "            after_target_word_index = index_target_word + 1\n",
        "            for x in range(after_target_word_index, after_target_word_index + window_size):\n",
        "                if x < len(corpus):\n",
        "                    context_words.extend([corpus[x]])\n",
        "\n",
        "        trgt_word_vector,ctxt_word_vector = get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index)\n",
        "        \n",
        "        # This check ensures that vectors are not just zeros\n",
        "        if np.sum(trgt_word_vector) > 0 and np.sum(ctxt_word_vector) > 0:\n",
        "            training_data.append([trgt_word_vector,ctxt_word_vector])\n",
        "\n",
        "        if sample is not None:\n",
        "            training_sample_words.append([target_word,context_words])   \n",
        "        \n",
        "    return training_data,training_sample_words\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "TOUOUya5fnqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(weight_inp_hidden, weight_hidden_output, target_word_vector):\n",
        "    # Compute the forward propagation step\n",
        "    hidden_layer = np.dot(weight_inp_hidden.T, target_word_vector)\n",
        "    output_layer = np.dot(weight_hidden_output.T, hidden_layer)\n",
        "\n",
        "    return softmax(output_layer), hidden_layer, output_layer\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "def calculate_error(y_pred, context_words):\n",
        "    # Compute the difference between predicted and actual context word vectors\n",
        "    error = y_pred - context_words\n",
        "    return error\n",
        "\n",
        "def backward_prop(weight_inp_hidden, weight_hidden_output, error, hidden_layer, target_word_vector, learning_rate):\n",
        "    # Compute the backward propagation step\n",
        "    dl_weight_inp_hidden = np.outer(target_word_vector, np.dot(weight_hidden_output, error.T))\n",
        "    dl_weight_hidden_output = np.dot(hidden_layer, error.T)\n",
        "\n",
        "    # Update weights\n",
        "    weight_inp_hidden = weight_inp_hidden - (learning_rate * dl_weight_inp_hidden)\n",
        "    weight_hidden_output = weight_hidden_output - (learning_rate * dl_weight_hidden_output)\n",
        "\n",
        "    return weight_inp_hidden, weight_hidden_output\n"
      ],
      "metadata": {
        "id": "wIa5p2HhgSxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def calculate_loss(u, ctx):\n",
        "    # We will calculate the negative log likelihood as it is used for multi-class classification problems\n",
        "    sum_1 = -1 * np.sum(u[ctx == 1])\n",
        "    sum_2 = len(np.where(ctx == 1)[0]) * np.log(np.sum(np.exp(u)))\n",
        "    total_loss = sum_1 + sum_2\n",
        "    return total_loss\n",
        "\n",
        "def main():\n",
        "    text = get_file_data()\n",
        "    word_to_index, index_to_word, corpus, vocab_size, length_of_corpus = generate_dictionary_data(text)\n",
        "\n",
        "    # Define your window size and sample here\n",
        "    window_size = 2  \n",
        "    sample = None \n",
        "\n",
        "    training_data, training_sample_words = generate_training_data(corpus, window_size, vocab_size, word_to_index, length_of_corpus, sample)\n",
        "\n",
        "    # Define the size of the hidden layer\n",
        "    hidden_layer_size = 50\n",
        "\n",
        "    # Initialize the weights\n",
        "    weight_inp_hidden = initialize_weights(vocab_size, hidden_layer_size)\n",
        "    weight_hidden_output = initialize_weights(hidden_layer_size, vocab_size)\n",
        "\n",
        "    # Define number of epochs for training\n",
        "    epochs = 50\n",
        "\n",
        "    # Define learning rate\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for target, context in training_data:\n",
        "            # Forward propagation\n",
        "            y_pred, h, u = forward_prop(weight_inp_hidden, weight_hidden_output, target)\n",
        "            \n",
        "            # Calculate error\n",
        "            EI = calculate_error(y_pred, context)\n",
        "            \n",
        "            # Sum up the error\n",
        "            total_error += np.sum(EI)\n",
        "            \n",
        "            # Backward propagation\n",
        "            weight_inp_hidden, weight_hidden_output = backward_prop(EI, h, weight_inp_hidden, weight_hidden_output, target, learning_rate)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, Error: {total_error}\")\n",
        "\n",
        "def initialize_weights(input_layer_size, hidden_layer_size):\n",
        "    # Initialize the weights randomly\n",
        "    weight_matrix = np.random.rand(input_layer_size, hidden_layer_size)\n",
        "    return weight_matrix\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "con4VXyAgbC2",
        "outputId": "ec5eb5c3-a3a4-4ac4-a5da-2c193ed02b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2ed5762860c7>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-2ed5762860c7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Backward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mweight_inp_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_hidden_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_inp_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_hidden_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch+1}, Error: {total_error}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-25993fa14db2>\u001b[0m in \u001b[0;36mbackward_prop\u001b[0;34m(weight_inp_hidden, weight_hidden_output, error, hidden_layer, target_word_vector, learning_rate)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Compute the backward propagation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdl_weight_inp_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_word_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_hidden_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdl_weight_hidden_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (50,395) and (50,395) not aligned: 395 (dim 1) != 50 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_embeddings(word_to_index, embeddings):\n",
        "    words = list(word_to_index.keys())\n",
        "    vectors = [embeddings[word_to_index[word]] for word in words]\n",
        "    \n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    vectors_2d = tsne.fit_transform(vectors)\n",
        "    \n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i, word in enumerate(words):\n",
        "        plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1])\n",
        "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_embeddings(word_to_index, weight_inp_hidden)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "kqqU9v8QlLZu",
        "outputId": "8fd43990-42f5-4eec-a6b1-caa43a7b4d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-88ae3b5e3589>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Assuming you have your word embeddings in 'input_hidden_weights' variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mvisualize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_inp_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'weight_inp_hidden' is not defined"
          ]
        }
      ]
    }
  ]
}